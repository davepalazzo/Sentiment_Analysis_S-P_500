{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "#from cleanMDA import extractTable, divide_chunks, pullMDA\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company name and text in dataframe\n",
    "df_sentences = pd.read_pickle('Cleaned_MDA_sentences.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get GDP for 2007-2017 (import from GitHut)\n",
    "def get_GDP_visual(df):\n",
    "    USA = np.array(df[df['Data Source'] == 'United States'])\n",
    "\n",
    "    gdp_USA = pd.DataFrame(USA, columns=['Country Name', 'Country COde', 'Indicator Name', 'Indicator Code',\n",
    "                                                 '1960','1961','1962','1963','1964','1965','1966','1967','1968','1969',\n",
    "                                                 '1970','1971','1972','1973','1974','1975','1976','1977','1978','1979',\n",
    "                                                 '1980','1981','1982','1983','1984','1985','1986','1987','1988','1989',\n",
    "                                                 '1990','1991','1992','1993','1994','1995','1996','1997','1998','1999',\n",
    "                                                 '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',\n",
    "                                                 '2010','2011','2012','2013','2014','2015','2016','2017','2018'])\n",
    "\n",
    "    gdp_USA = gdp_USA.drop(columns=['Country Name','Country COde','Indicator Name','Indicator Code','1960','1961','1962',\n",
    "                                   '1963','1964','1965','1966','1967','1968','1969', '1970','1971','1972','1973','1974',\n",
    "                                   '1975','1976','1977','1978','1979', '1980','1981','1982','1983','1984','1985','1986',\n",
    "                                   '1987','1988','1989', '1990','1991','1992','1993','1994','1995','1996','1997','1998',\n",
    "                                   '1999', '2000','2001','2002','2003','2004','2005','2006','2018'])\n",
    "\n",
    "    gdp_USA_trans = gdp_USA.transpose()\n",
    "    gdp_USA_trans = gdp_USA_trans.rename(columns={0:'Y'})\n",
    "    \n",
    "    return gdp_USA_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df = pd.read_csv('gdp_annual copy.csv')\n",
    "gdp_USA = get_GDP_visual(gdp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1.77857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>-0.291621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>-2.77553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2.53192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>1.60145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2.22403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1.67733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2.56919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2.86159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1.48528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2.27334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y\n",
       "2007   1.77857\n",
       "2008 -0.291621\n",
       "2009  -2.77553\n",
       "2010   2.53192\n",
       "2011   1.60145\n",
       "2012   2.22403\n",
       "2013   1.67733\n",
       "2014   2.56919\n",
       "2015   2.86159\n",
       "2016   1.48528\n",
       "2017   2.27334"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis_sentence(text):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    # split for each sentence\n",
    "    for word in text.split(' . '):\n",
    "        blob = TextBlob(word)\n",
    "        sent = blob.sentiment\n",
    "        # eliminate words with high subjectivity\n",
    "        if sent.subjectivity < 0.7:\n",
    "            if sent.polarity > 0.25:\n",
    "                pos.append(sent.polarity)\n",
    "            elif sent.polarity < -0.25:\n",
    "                neg.append(sent.polarity)\n",
    "    return pos,neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df to run an analysis of each company's text\n",
    "analysis_df = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])\n",
    "log_analysis = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "for year in df_sentences:\n",
    "    cols = []\n",
    "    cols_log = []\n",
    "    for i in range(len(df_sentences)):\n",
    "        sum_pos = 0\n",
    "        sum_neg = 0\n",
    "        # combined the list of text words into a string\n",
    "        pos, neg = text_analysis(' '.join(df_sentences[year][i][1]))\n",
    "        # keep track of the positive and negitive total\n",
    "        for i in pos:\n",
    "            sum_pos += i\n",
    "\n",
    "        for j in neg:\n",
    "            sum_neg += j\n",
    "\n",
    "        total = sum_pos+sum_neg\n",
    "        cols.append(total)\n",
    "        cols_log.append(np.log(total))\n",
    "    # put scores in df by year\n",
    "    analysis_df[year] = cols\n",
    "    log_analysis[year] = cols_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis_df.transpose()\n",
    "analysis_df['y'] = np.array(gdp_USA['Y'])\n",
    "\n",
    "log_analysis = log_analysis.transpose()\n",
    "log_analysis['y'] = np.array(gdp_USA['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_analysis = log_analysis.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the Xs into one value\n",
    "comb_Xs = np.array(analysis_df.iloc[:,0:10].sum(axis=1))\n",
    "single_df = pd.DataFrame(index=df_sentences.columns)\n",
    "single_df['X'] = comb_Xs\n",
    "single_df['y'] = analysis_df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "#X = np.array(analysis_df.iloc[:,0:10])\n",
    "#y = np.array(analysis_df['y'])\n",
    "#X = np.array(single_df['X'])\n",
    "#y = np.array(single_df['y'])\n",
    "X = np.array(log_analysis.iloc[:,0:10])\n",
    "y = np.array(log_analysis['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if combined Xs dataframe\n",
    "X_train = X_train[:, np.newaxis]\n",
    "X_test = X_test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run analysis on multiple models\n",
    "xgb_reg = xgb.XGBRegressor(max_depth=4,n_estimators=300, n_jobs=-1, subsample=.7,random_seed=3)\n",
    "lr = LinearRegression()\n",
    "reg = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "-1915.028634926607\n",
      "[19.10038288 10.50797337]\n",
      "12.23671534286009\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "-21.598048990829078\n",
      "[1.20015432 1.2011848 ]\n",
      "1.36679322689747\n",
      "[14:38:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=4, min_child_weight=1, missing=None, n_estimators=300,\n",
      "       n_jobs=-1, nthread=None, objective='reg:linear', random_seed=3,\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=None, subsample=0.7, verbosity=1)\n",
      "-7.148147725233445\n",
      "[3.4190795 1.2250164]\n",
      "0.8029073548227539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.86158702, 2.27333855])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for m in [lr,reg,xgb_reg]:\n",
    "    # model learns from train data\n",
    "    m.fit(X_train,y_train)\n",
    "    print(m)\n",
    "    print(m.score(X_test,y_test))\n",
    "    y_pred = m.predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis_word(text):\n",
    "    score = []\n",
    "    for word in text:\n",
    "        blob = TextBlob(word)\n",
    "        sent = blob.sentiment\n",
    "        # eliminate words with high subjectivity\n",
    "        if sent.subjectivity < 0.7:\n",
    "            if sent.polarity > 0.0:\n",
    "                score.append(sent.polarity)\n",
    "            elif sent.polarity < 0.0:\n",
    "                score.append(sent.polarity)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analysis_df = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])\n",
    "word_log_analysis = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-53ee34046218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msum_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_analysis_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# keep track of the positive and negitive total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-417-60cd7d0a739e>\u001b[0m in \u001b[0;36mtext_analysis_word\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# eliminate words with high subjectivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjectivity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/textblob/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36msentiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnamedtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \"\"\"\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/textblob/en/sentiments.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text, keep_assessments)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mSentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subjectivity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpattern_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/collections/__init__.py\u001b[0m in \u001b[0;36mnamedtuple\u001b[0;34m(typename, field_names, rename, defaults, module)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mfield_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mfield_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mtypename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in df_sentences:\n",
    "    cols = []\n",
    "    cols_log = []\n",
    "    for i in range(len(df_sentences)):\n",
    "        sum_total = 0\n",
    "        score = text_analysis_word(df_sentences[year][i][1])\n",
    "        # keep track of the positive and negitive total\n",
    "        for j in score:\n",
    "            sum_total += j\n",
    "\n",
    "        cols.append(sum_total)\n",
    "        cols_log.append(np.log(sum_total))\n",
    "        \n",
    "    # put scores in df by year\n",
    "    word_analysis_df[year] = cols\n",
    "    word_log_analysis[year] = cols_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
