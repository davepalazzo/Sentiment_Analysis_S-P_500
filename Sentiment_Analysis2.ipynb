{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "#from cleanMDA import extractTable, divide_chunks, pullMDA\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company name and text in dataframe\n",
    "df_sentences = pd.read_pickle('Cleaned_MDA_sentences.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get GDP for 2007-2017 (import from GitHut)\n",
    "def get_GDP_visual(df):\n",
    "    USA = np.array(df[df['Data Source'] == 'United States'])\n",
    "\n",
    "    gdp_USA = pd.DataFrame(USA, columns=['Country Name', 'Country COde', 'Indicator Name', 'Indicator Code',\n",
    "                                                 '1960','1961','1962','1963','1964','1965','1966','1967','1968','1969',\n",
    "                                                 '1970','1971','1972','1973','1974','1975','1976','1977','1978','1979',\n",
    "                                                 '1980','1981','1982','1983','1984','1985','1986','1987','1988','1989',\n",
    "                                                 '1990','1991','1992','1993','1994','1995','1996','1997','1998','1999',\n",
    "                                                 '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',\n",
    "                                                 '2010','2011','2012','2013','2014','2015','2016','2017','2018'])\n",
    "\n",
    "    gdp_USA = gdp_USA.drop(columns=['Country Name','Country COde','Indicator Name','Indicator Code','1960','1961','1962',\n",
    "                                   '1963','1964','1965','1966','1967','1968','1969', '1970','1971','1972','1973','1974',\n",
    "                                   '1975','1976','1977','1978','1979', '1980','1981','1982','1983','1984','1985','1986',\n",
    "                                   '1987','1988','1989', '1990','1991','1992','1993','1994','1995','1996','1997','1998',\n",
    "                                   '1999', '2000','2001','2002','2003','2004','2005','2006','2018'])\n",
    "\n",
    "    gdp_USA_trans = gdp_USA.transpose()\n",
    "    gdp_USA_trans = gdp_USA_trans.rename(columns={0:'Y'})\n",
    "    \n",
    "    return gdp_USA_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df = pd.read_csv('gdp_annual.csv')\n",
    "gdp_USA = get_GDP_visual(gdp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1.77857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>-0.291621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>-2.77553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2.53192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>1.60145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2.22403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1.67733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2.56919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2.86159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1.48528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2.27334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y\n",
       "2007   1.77857\n",
       "2008 -0.291621\n",
       "2009  -2.77553\n",
       "2010   2.53192\n",
       "2011   1.60145\n",
       "2012   2.22403\n",
       "2013   1.67733\n",
       "2014   2.56919\n",
       "2015   2.86159\n",
       "2016   1.48528\n",
       "2017   2.27334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis_sentence(text, subjective=True):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    # split for each sentence\n",
    "    for word in text.split(' . '):\n",
    "        blob = TextBlob(word)\n",
    "        sent = blob.sentiment\n",
    "        # eliminate words with high subjectivity\n",
    "#         if subjective:\n",
    "#             if sent.subjectivity < 0.7:\n",
    "#                 if sent.polarity > 0.25:\n",
    "#                     pos.append(sent.polarity)\n",
    "#                 elif sent.polarity < -0.25:\n",
    "#                     neg.append(sent.polarity)\n",
    "        \n",
    "    return word,sent.polarity,sent.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df to run an analysis of each company's text\n",
    "analysis_df = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])\n",
    "log_analysis = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "for year in df_sentences:\n",
    "    cols = []\n",
    "    cols_log = []\n",
    "    for i in range(len(df_sentences)):\n",
    "        sum_pos = 0\n",
    "        sum_neg = 0\n",
    "        # combined the list of text words into a string\n",
    "        pos, neg = text_analysis_sentence(' '.join(df_sentences[year][i][1]))\n",
    "        # keep track of the positive and negitive total\n",
    "        for i in pos:\n",
    "            sum_pos += i\n",
    "\n",
    "        for j in neg:\n",
    "            sum_neg += j\n",
    "\n",
    "        total = sum_pos+sum_neg\n",
    "        cols.append(total)\n",
    "        cols_log.append(np.log(total))\n",
    "    # put scores in df by year\n",
    "    analysis_df[year] = cols\n",
    "    log_analysis[year] = cols_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis_df.transpose()\n",
    "analysis_df['y'] = np.array(gdp_USA['Y'])\n",
    "\n",
    "log_analysis = log_analysis.transpose()\n",
    "log_analysis['y'] = np.array(gdp_USA['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_analysis = log_analysis.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the Xs into one value\n",
    "comb_Xs = np.array(analysis_df.iloc[:,0:10].sum(axis=1))\n",
    "single_df = pd.DataFrame(index=df_sentences.columns)\n",
    "single_df['X'] = comb_Xs\n",
    "single_df['y'] = analysis_df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "kf = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "#X = np.array(analysis_df.iloc[:,0:10])\n",
    "#y = np.array(analysis_df['y'])\n",
    "#X = np.array(single_df['X'])\n",
    "#y = np.array(single_df['y'])\n",
    "X = np.array(log_analysis.iloc[:,0:10])\n",
    "y = np.array(log_analysis['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if combined Xs dataframe\n",
    "X_train = X_train[:, np.newaxis]\n",
    "X_test = X_test[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run analysis on multiple models\n",
    "xgb_reg = xgb.XGBRegressor(max_depth=4,n_estimators=300, n_jobs=-1, subsample=.7,random_seed=3)\n",
    "lr = LinearRegression()\n",
    "reg = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3ed6dcc1e961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxgb_reg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# model learns from train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 458\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 570\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "for m in [lr,reg,xgb_reg]:\n",
    "    # model learns from train data\n",
    "    m.fit(X_train,y_train)\n",
    "    print(m)\n",
    "    print(m.score(X_test,y_test))\n",
    "    y_pred = m.predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis_word(df):\n",
    "    score = []\n",
    "    for word in text:\n",
    "        blob = TextBlob(word)\n",
    "        sent = blob.sentiment\n",
    "        # eliminate words with high subjectivity\n",
    "        if sent.subjectivity < 0.7:\n",
    "            if sent.polarity > 0.0:\n",
    "                score.append(sent.polarity)\n",
    "            elif sent.polarity < 0.0:\n",
    "                score.append(sent.polarity)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analysis_df = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])\n",
    "word_log_analysis = pd.DataFrame(index=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "for year in df_sentences:\n",
    "    cols = []\n",
    "    cols_log = []\n",
    "    for i in range(len(df_sentences)):\n",
    "        sum_total = 0\n",
    "        score = text_analysis_word(df_sentences[year][i][1])\n",
    "        # keep track of the positive and negitive total\n",
    "        for j in score:\n",
    "            sum_total += j\n",
    "\n",
    "        cols.append(sum_total)\n",
    "        cols_log.append(np.log(sum_total))\n",
    "        \n",
    "    # put scores in df by year\n",
    "    word_analysis_df[year] = cols\n",
    "    word_log_analysis[year] = cols_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
